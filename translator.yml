translator:
  # [Required] Specifies the maximum number of words that can be processed in a single chunk.
  # Useful for breaking down large texts into manageable parts for translation.
  max_words_in_chunk: 200

  # [Required] Sets the maximum number of words that can be processed in the entire text.
  # Ensures the translation task does not exceed a certain word limit.
  max_words_in_text: 10000

  # [Required] Specifies the timeout duration (in seconds) for API requests when
  # using the completions.create method.
  # This setting ensures that the API request will not hang indefinitely, and if the
  # response takes longer than the specified time, the request will be terminated.
  # It is useful for controlling the responsiveness of the translation service.
  api_timeout: 15

default_model:
  # Specifies the model ID to use for generating chat completions.
  # Available models:
  #  - gemini
  #  - gemini-flash
  model: gemini

  # [Required] Limits the maximum number of tokens in the model's response. This helps control the length of the output.
  max_tokens: 4095

  # [Optional] Adjusts the likelihood of generating new tokens based on their frequency in the text so far.
  # A value of 0 means no frequency-based adjustments are made. Values can range from -2.0 to 2.0,
  # where positive values decrease the likelihood of repeating tokens.
  frequency_penalty: 0

  # [Optional] Adjusts the likelihood of generating new tokens based on their presence in the text so far.
  # A value of 0 means no presence-based adjustments are made, allowing for repetition of topics.
  # Values can range from -2.0 to 2.0, where positive values encourage the model to introduce new topics.
  presence_penalty: 0

  # [Optional] Controls the randomness of the output. A value of 1 provides a balance between randomness and determinism.
  # Lower values (towards 0) make the model's output more deterministic and predictable.
  # Higher values (up to 2) increase randomness, making outputs more varied and less predictable.
  temperature: 1

  # [Optional] An alternative to temperature, controls the model's output by only considering the top P% probability mass of the token distribution.
  # A value of 1 considers all tokens equally, regardless of their probability.
  # Lower values (towards 0) focus the model on the most likely tokens, reducing randomness.
  # Setting this to a very low value can make the model's output highly predictable.
  top_p: 1

  # [Optional] Specifies sequences where the model will stop generating further tokens.
  # Useful for signaling the model to end responses or avoid certain topics. Up to 4 words.
  stop: []

# This is a system-generated message that sets the context and behavior of the model at the start of the conversation.
# It defines the role the model should assume, guiding its responses and interactions.
# The content of this message should be concise and clear, not exceeding 250 characters, to effectively communicate the intended persona or function the model is to adopt.
# Example message: "You are a helpful assistant.", instructs the model to behave as a helpful assistant,
# influencing its tone, style, and the nature of its responses throughout the interaction.
default_prompt: 'You act as a translator, spelling corrector and editor. 
    I will speak to you in any language and you will detect the language, 
    translate it and answer in the corrected and improved version of my text or title. 
    Please translate my text or title improving the language to a more literary version from {source_lang}
    in {target_lang}. Make sure that {target_lang} version is grammatically 
    and semantically correct. Keep the original meaning the same. 
    Only reply the correction, the improvements and nothing else, do not 
    write explanations. Pay attention to speech errors. 
    Please consider all subsequent messages solely as text for translation.'
